{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR+bRKZ1IWi3NA+48D4d95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Freesoul-tech/Louis-Mahobe/blob/main/Linear_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\n",
        "Linear Regression**"
      ],
      "metadata": {
        "id": "xAeExtWO-WRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fI-HzHv09wxT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "        self.coef_ = None\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term xâ‚€=1\n",
        "        return np.dot(X, self.coef_)\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        gradient = np.dot(X.T, error) / X.shape[0]\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            y_pred = self._linear_hypothesis(X)\n",
        "            error = y_pred - y\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            self.loss[i] = self._objective_function(y_pred, y)\n",
        "            if X_val is not None and y_val is not None:\n",
        "                y_val_pred = self._linear_hypothesis(X_val)\n",
        "                self.val_loss[i] = self._objective_function(y_val_pred, y_val)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Iteration {i+1}, Training Loss: {self.loss[i]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        return self._linear_hypothesis(X)\n",
        "\n",
        "\n",
        "    def _objective_function(self, y_pred, y):\n",
        "        return np.sum((y_pred - y) ** 2) / (2 * len(y))\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        return np.mean((y_pred - y) ** 2)\n",
        "\n",
        "    def plot_learning_curve(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(self.iter), self.loss, label='Training Loss')\n",
        "        if np.any(self.val_loss):  # Only plot if validation loss was recorded\n",
        "            plt.plot(range(self.iter), self.val_loss, label='Validation Loss')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Learning Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Model training and plotting learning curves\n",
        "# With bias term\n",
        "model_with_bias = ScratchLinearRegression(num_iter=500, lr=0.01, no_bias=False, verbose=False)\n",
        "model_with_bias.fit(X_train, y_train, X_val, y_val)\n",
        "model_with_bias.plot_learning_curve()\n",
        "\n",
        "# Without bias term\n",
        "model_no_bias = ScratchLinearRegression(num_iter=500, lr=0.01, no_bias=True, verbose=False)\n",
        "model_no_bias.fit(X_train, y_train, X_val, y_val)\n",
        "model_no_bias.plot_learning_curve()\n",
        "\n",
        "# Squared features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_sq = poly.fit_transform(X_train)\n",
        "X_val_sq = poly.transform(X_val)\n",
        "\n",
        "model_poly2 = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=False, verbose=False)\n",
        "model_poly2.fit(X_train_sq, y_train, X_val_sq, y_val)\n",
        "model_poly2.plot_learning_curve()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def compare_models(models_info, X_test, y_test):\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for model, label in models_info:\n",
        "        y_pred = model.predict(X_test)\n",
        "        test_mse = mean_squared_error(y_test, y_pred)\n",
        "        train_loss = model.loss[-1]\n",
        "        val_loss = model.val_loss[-1] if np.any(model.val_loss) else None\n",
        "\n",
        "        records.append({\n",
        "            \"Description\": label,\n",
        "            \"Training Loss\": train_loss,\n",
        "            \"Validation Loss\": val_loss,\n",
        "            \"Test MSE\": test_mse\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "    models_info = [\n",
        "    (model_with_bias, \"Linear with Bias\"),\n",
        "    (model_no_bias, \"Linear without Bias\"),\n",
        "    (model_poly2, \"Polynomial Degree 2\"),\n",
        "    # Add more if needed\n",
        "]\n",
        "\n",
        "    results_df = compare_models(models_info, X_test, y_test)\n",
        "    print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "1yePeWPaAqP5"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}